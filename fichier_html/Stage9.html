<!DOCTYPE html><html lang="fr"><head><meta charset="UTF-8"><title> CartoStages </title></head><body><p>Titre: Generation of natural language datasets and metrics of representativeness</p><p>Date: 2023-01-11</p><p>Organisme: INRIA</p><p>Lieu: Paris / Saclay / Rocquencourt</p><br/><p>Research intern position (with the goal of pursuing in PhD) at Inria on generation of natural language datasets and metrics of representativeness</p><p></p><p>*Place of work*: Inria center in Paris area (Paris / Saclay /Rocquencourt)</p><p></p><p>*Duration*: 6 months internship + 3 years PhD</p><p></p><p>*Starting date*: Anytime in 2023</p><p></p><p>*Keywords*: artificial intelligence, natural language processing, natural language generation, story generation, evaluation metrics</p><p></p><p>*Context*</p><p>This internship fits within the roadmap activities of Inria's Defense & Security Department.</p><p></p><p>Inria's Defense & Security Department develops and maintains a serious game platform which can simulate the activity of a crisis monitoring cell. For instance, analysts in geopolitical crises are employed by the</p><p>French Ministry of Armed Forces to better identify emerging or ongoing conflicts throughout the world. These analysts are typically overwhelmed by continuous streams of plain-text information and can be helped by Natural Language Processing (NLP) tooling. That platform enables, among other uses, to experiment with the NLP tools developed by researchers and partners, in order to get practical feedback from players on their usefulness during action.</p><p></p><p>For these simulations, a fictive world has been created by Inria, including imaginary cities and an imaginary historical, social and political background.</p><p></p><p>In order to test NLP solutions in a game scenario, it is necessary to create large amounts of text documents, which need to be both realistic in terms of their form and type of contents, and tailored to the context of that imaginary world. To date, this creation remains mostly manual, which is a very time-consuming process. To scale the scenarios up, it is thus planned to gradually automate this data creation, which is the ambition to which this internship will contribute.</p><p></p><p>The intern will be supervised by Dr Lauriane Aufrant, who is the lead</p><p>NLP researcher within Inria's Defense & Security Department. PhD</p><p>supervision will be done jointly with Dr Frédérique Segond (Inria's</p><p>Defense & Security Director) or with a researcher from another Inria team, depending on the exact chosen PhD topic (to be discussed, see below).</p><p></p><p></p><p>*Candidate profile*</p><p></p><p>-   Pursuing a master's degree in Natural Language Processing,</p><p>    Computational Linguistics or Computer Science with a specialization</p><p>    in Machine Learning</p><p>-   Theoretical and practical knowledge of deep learning, as well as</p><p>    traditional machine learning and knowledge-driven AI</p><p>-   Strong programming skills (at least Python, git, Linux environment,</p><p>    command line and scripting)</p><p>-   Fluency in English. Knowledge or interest for the French language.</p><p>    Knowledge of a second foreign language would be appreciated.</p><p></p><p></p><p>*How to apply*</p><p></p><p>Send a CV and a cover letter to lauriane.aufrant and frederique.segond (both at inria.fr)</p><p>Indications of referees or reference letters would be appreciated but are not mandatory.</p><p></p><p></p><p>*Internship description*</p><p></p><p>Two configurations for Natural Language Generation (NLG) will be considered throughout this work: topic-based NLG (to produce haystacks) and knowledge base-to-text NLG (to produce needles).</p><p></p><p>The internship will be devoted to prepare the scientific framework within which the PhD work will be developed, and in particular metrics that will be used to guide and evaluate future contributions on the generation itself.</p><p></p><p>In the current NLG literature, text generation is usually evaluated at the sentence- or document-level only, by considering aspects such as fluency (is it good French?) or coherence (does the text make sense?).</p><p>But much more rarely is the setting of NLG evaluation at the dataset level considered. This includes accounting for higher-level properties, such as realism, consistency and diversity:</p><p>-   Diversity, as in generating documents with sufficient variability</p><p>    in form and content, is increasingly considered in the literature,</p><p>    in particular as a result of some NLG models' tendency to repeat</p><p>    themselves. However, existing metrics are often rather basic, and</p><p>    covering only some aspects of the diversity property (see for</p><p>    instance https://arxiv.org/pdf/2006.14799.pdf or</p><p>    https://aclanthology.org/2021.eacl-main.25.pdf ), so there is room</p><p>    for improvement.</p><p>-   Consistency, as in avoiding to generate one document where a given</p><p>    person is born on April 12 and another where the same person is</p><p>    born on April 13, is sometimes considered at the level of a single</p><p>    document (in the context of story generation), but not really</p><p>    across documents. Still, there is inspiration to be drawn here from</p><p>    the fact checking literature, for instance considering settings</p><p>    where each document in turn is to be fact checked against all</p><p>    others.</p><p>-   Realism is a much more complex, multi-faceted property,</p><p>    encompassing at least the adequacy of style (would the fictive</p><p>    author have written that way, in light of their background?), the</p><p>    nature of contents (would a transcript of political debate cite</p><p>    football match results?), the nature of facts (would a politician</p><p>    be aged 15, or a World cup football player aged 60?), and possibly</p><p>    many other aspects. On this part, most of the work remains to be</p><p>    done.</p><p></p><p>The intern's work will be to develop new metrics to quantify dataset quality along those three properties, drawing inspiration both from conceptual investigations and user studies (to build a taxonomy of aspects to evaluate) and from empirical studies conducted with state-of-the-art NLG models to observe how the generated data reacts to various tentative metrics.</p><p></p><p></p><p>*PhD follow-up*</p><p></p><p>Based on the scientific methodology developed during the internship to evaluate NLG quality at dataset level, the work will be pursued as a</p><p>PhD to propose new generation methods that better perform along those metrics.</p><p></p><p>Considering both settings of topic-based and knowledge base-to-text</p><p>NLG, the goal in each case is to compare pure generation methods (using</p><p>GPT3-like models or more specialized models) with approaches based on well-focused Web crawling followed by text substitutions, paraphrasing and other automated transformations applied on the collected documents, to modify their style or their base information.</p><p></p><p>The PhD student will work on designing and implementing new approaches along those lines, but also use the new metrics to evaluate and compare those approaches with existing models in NLG literature. As work progresses, it is also likely that the initially proposed metrics are refined or complemented to better match empirical evidence gathered during the PhD.</p><p></p><p>The exact PhD topic will be written in coordination with the intern, to fit their primary interests within that broad objective. In any case, the work will focus as a first step on plain-text documents written in correct language, but then it will tackle the generation of more complex types of data, for which several options can be considered, such as: generating tweets, documents with corrupted language (simulating typos or grammar errors in the most realistic way), multimodal documents that include and discuss tables or pictures, corpora containing divergent views and (purposedly) inconsistent facts, etc.</p><p></p><p>Since the PhD application processes are early in the year (February-April), the intern will be asked to commit early to that PhD</p><p>follow-up, possibly even before the internship begins, and to be ready to devote some time for writing the application over that period.</p><p></p></body></html>