<!DOCTYPE html><html lang="fr"><head><meta charset="UTF-8"><title> CartoStages </title></head><body><p>Titre: Les disfluences verbales et marqueurs discursifs : impact sur le traitement automatique de la parole et améliorations liées</p><p>Date: 2023-01-30</p><p>Organisme: Linagora</p><p>Lieu: Toulouse</p><br/><p> *Les disfluences verbales et marqueurs discursifs :</p><p>Impact sur le traitement automatique de la parole et améliorations liées*</p><p></p><p>L'équipe R&D (_https://labs.linagora.com/_) de la société *LINAGORA*(_http://linagora.com_) développe en open-source des outils d'assistance intelligente pour entreprises, y compris l'assistant vocal LinTO (_https://linto.ai/_) et la bibliothèque d'outils associée (https://github.com/linto-ai), dont le focus est de mettre à disposition l'état de l'art en Reconnaissance</p><p>Automatique de la Parole (RAP). La parole conversationnelle intéresse tout particulièrement LINAGORA, avec comme application cible le *résumé automatique de réunion*. Cette application met en scène la RAP</p><p>ainsi que le Traitement Automatique du Langage Naturel (TALN).</p><p></p><p>Les systèmes modernesde tâches de TALN, tel la restitution de la ponctuation et la génération automatique de résumé, sont basés sur des modèles statistiques d'apprentissage automatique (*Machine Learning*) entraînés sur de grandes quantités dedonnées textuelles, extraites la plupart du temps de sites web et de recueils numérisés. Or ces données ne sont pas représentatives des transcriptions de *parole spontanée*.</p><p>De ce fait, les modèles de TALN appris sur ces données ne sont pas adaptés à des applications comme le résumé automatique de réunion à partir de transcriptions de RAP brutes. D'autre part, un système de résumé automatique nécessite d'importants volumes de données pour atteindre des performances acceptables, et il serait trop long et coûteux de recueillir assez de données transcrites de paroles spontanées pour qu'un système de TALN basé sur du Machine Learning puisse généraliser correctement.</p><p></p><p>La motivation de ce stage s'appuie sur le constat que, si l'on met de côté les erreurs de transcription de la RAP (qui deviennent raresau fur et à mesure des progrès en RAP), deuxdes plus grandesdifférencesentre la transcription de parole spontanée et le texte écrit est :</p><p></p><p>-   d'un côté, la présence de disfluences verbales, à savoirles</p><p>    hésitations (« euh... », « hmm »), les répétitions, et lesfaux</p><p>    départs,</p><p></p><p>-   d'un autre côté, la présencede marqueurs discursifs (« eh bien »,</p><p>    « alors », « donc »...), qui sont souvent employés pour réguler le</p><p>    flux de parole.</p><p></p><p>Non seulement les disfluences et les marqueurs discursifspeuvent être importants pour les tâches en aval nécessitant une compréhension du langage (comme la segmentation discursive), mais ils sont présents dans tout corpus utilisé pour former de nouveaux modèles de RAPet devront donc être pris en compte lors de la création d'une vérité terrain. Dans ce qui suit, pour des raisons de simplicité, nous parlons de « disfluences » pour désigner à la fois les disfluences et les marqueurs discursifs, bien qu'il soit de manière générale utile de différencier les deux.</p><p></p><p>Il n'existe pas ou très peu de bases de données annotant les disfluences au sein de discours. Or, les modèles« Whisper » d'OpenAI, à l'état de l'art en RAP etappris de manière semi-supervisée sur un très grand volume de vidéos sous-titrées, présententlaparticularité d'omettre les disfluences, dans certaines conditions qu'ilreste à déterminer. Ils font par ailleurspreuve d'une étonnanterobustesse dansplusieurs langues, dont l'anglais et le français.</p><p></p><p>Partant de ce constat, le premier objectif du stage sera de constituer une base de données avec annotation des disfluences dans des transcriptions de parole. La constitution de cette base se ferade manière automatique à partir d'un programme, élaboré par le stagiaire, qui consiste à appliquerles modèles Whisper aux bases de données vocalesdisponibles à LINAGORA, et à exploiterles alignements de ses transcriptions (incomplètes en termes de disfluences) avec la vérité terrain.</p><p></p><p>À partir de cette base de donnée, le second objectifsera d'entraîner un modèle « deep learning » de TALN permettant de détecter et supprimer les disfluences dans les transcriptions textuelles. Selon l'avancement du stage, il sera aussi possible d'entraîner un modèle permettant de rajouter des disfluences dans un texte, dont la principale utilité estd'augmenter les bases de données d'entraînement des modèles TALN</p><p>pour qu'ils soient adaptés au langage parlé.</p><p></p><p>Le modèle de détection/suppressiondes disfluences sera utilisé pour améliorer les performances des systèmes de TALN appliqués à la parole.</p><p>En plus de vérifier et mesurer cette amélioration de performances en</p><p>TALN, le stagiaire pourra utiliser ce modèle pour analyser l'impact des disfluences sur les systèmes de RAP. En particulier, une des utilités d'un tel modèle est d'améliorer l'estimation des performances des systèmes de RAP, pour comparer les systèmes qui transcrivent les disfluences et ceux qui les omettent.</p><p></p><p>*E**ncadrement du stage* :</p><p>Le stagiaire sera encadré par Jérôme Louradour et Julie Hunter de</p><p>LINAGORA.</p><p></p><p>*Localisation* : LINAGORA GSO, Toulouse</p><p></p><p>*Compétences clés recherchées* :</p><p></p><p>-   Étudiants de M2 ou d'école d'ingénieur en dernière année, en</p><p>    informatique, avec des compétences en machine learning</p><p></p><p>-   De l'expérience en deep learning serait un plus</p><p></p><p>-   De l'expérience en traitement de la parole et/ou du texte serait un</p><p>    plus</p><p></p><p>*Durée du stage* : 5-6 mois</p><p></p><p>*Gratification* : à définir selon l'expérience du candidat</p><p></p><p>*Contact**s**email* : jlouradour@linagora.com, jhunter@linagora.com, jplorre@linagora.com</p><p></p><p></p><p>*Références :*</p><p></p><p>-   « La parole spontanée : transcription et traitement », Thierry</p><p>    Bazillon, Vincent Jousse, Frédéric Béchet, Yannick Estève, Georges</p><p>    Linarès, Daniel Luzzati</p><p></p><p>-   « Auto-interruptions et disfluences en français parlé dans quatre</p><p>    corpus du CID », Bertille Pallaud, Stéphane Rauzy et Philippe</p><p>    Blache</p><p></p><p>-   « Analyse et détection automatique de disfluences dans la parole</p><p>    spontanée conversationnelle », Camille Dutrey</p><p></p><p>-   (Whisper) « Robust speech recognition via large-scale weak</p><p>    supervision », Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,</p><p>    Christine McLeavey et Ilya Sutskever</p><p></p><p></p><p></p></body></html>