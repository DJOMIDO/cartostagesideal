<!DOCTYPE html><html lang="fr"><head><meta charset="UTF-8"><title> CartoStages </title></head><body><p>Titre: Exploiting the structure of HTML to learn document representation</p><p>Date: 2023-01-18</p><p>Organisme: Qwant & ISIR</p><p>Lieu: Paris</p><br/><p> # Exploiting the structure of HTML to learn document representations</p><p></p><p>## Context</p><p>Information Retrieval (IR) models aim at predicting which documents within a potentially huge collection are relevant to a given user information need (usually a query). Current models of Information</p><p>Retrieval, like in many other fields, are nowadays based on transformer architectures.</p><p></p><p>More precisely, two types of model are now prevalent: (1) representation-based techniques, where the document and the query representations are computed separately (dense or sparse vector) before using a matching scoring function (e.g. inner product); (2) interaction-based techniques, where both the query and the document content are used to compute a relevance score.</p><p></p><p>Current research focuses on how to (pre)train the models and the problem of modeling the task better, i.e., how to compute the representation of the document and/or the query, or of both the query and document. Improving the quality of the representation is key to building successful (transformer) models for IR, as shown in the best-performing models to date [Gao and Callan, 2021].</p><p></p><p>## Objectives</p><p>The internship will explore new ways to compute the representation of (Web) documents, by considering various aspects of Web documents, i.e.</p><p>both their internal (DOM) and external (links) structure in the context of Information Retrieval.</p><p></p><p>In the context of Web search, when dealing with web pages, the Document</p><p>Object Model (DOM) tree represents the document's structure [Gupta et al., 2003].  Recent work on transformer-based models shows that this structure can be encoded explicitly [Ainslie et al., 2020] or implicitly [Aghajanyan et al., 2021] in the model. One recent approach [Guo et al., 2022] proposes to separate the encoding of the text content from the node structure, before using both representations as a basis for dense ranking.</p><p></p><p>The goals of this internship will be to study how the HTML structure can be leveraged to (1) build better document representations by exploiting the inner HTML structure and/or the hyperlinks between the documents; and (2) provide a better pre-training (i.e. without the supervision of query paired with relevant documents).</p><p></p><p>The intern is encouraged to develop their own ideas, and to publish in (inter)national venues and/or to participate in international evaluation campaigns (such as TREC).</p><p></p><p>Organization</p><p>The internship will take place at the Qwant offices with visits to ISIR</p><p>(remote work is also possible). The internship is supervised by</p><p>Benjamin Piwowarski from ISIR, and Lara Perinetti and Romain Deveaud from Qwant.</p><p></p><p>The intern will potentially work with the following tools/technologies:</p><p>-   Deep Learning libraries (PyTorch, TensorFlow, Jax/Flax, Huggingface</p><p>    ecosystem, etc.)</p><p>-   Python</p><p>-   Search engine tools (https://github.com/vespa-engine/pyvespa)</p><p>-   Git version control</p><p>-   Jupyter Environment</p><p></p><p>Qwant will provide the intern a laptop and access to a remote compute server with GPU capabilities.</p><p></p><p>Candidates can send their questions, as well as their resumes + motivation (a few lines) to l.perinetti@qwant.com, r.deveaud@qwant.com and benjamin@piwowarski.fr</p><p></p><p></p><p>## References [Gao and Callan, 2021] L. Gao and J. Callan, "Unsupervised Corpus Aware</p><p>    Language Model Pre-training for Dense Passage Retrieval,"</p><p>    arXiv:2108.05540 [cs], Aug. 2021 [Online].</p><p>    Available: http://arxiv.org/abs/2108.05540 .</p><p></p><p>[Gupta et al., 2003] S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm,</p><p>    "DOM-based content extraction of HTML documents," in Proceedings of</p><p>    the twelfth international conference on World Wide Web - WWW '03,</p><p>    Budapest, Hungary, 2003, p. 207, doi: 10.1145/775152.775182 [Online].</p><p>    Available: http://portal.acm.org/citation.cfm?doid=775152.775182 .</p><p></p><p>[Ainslie et al., 2020] J. Ainslie et al., "ETC: Encoding Long and</p><p>    Structured Inputs in Transformers," in Proceedings of the 2020</p><p>    Conference on Empirical Methods in Natural Language Processing</p><p>    (EMNLP), Online, 2020, pp. 268-284,</p><p>    doi: 10.18653/v1/2020.emnlp-main.19 [Online].</p><p>    Available: https://www.aclweb.org/anthology/2020.emnlp-main.19 .</p><p></p><p>[Aghajanyan et al., 2021] Aghajanyan, Armen, Dmytro Okhonko, Mike</p><p>    Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettlemoyer.</p><p>    "HTLM: Hyper-Text Pre-Training and Prompting of Language Models."</p><p>    ArXiv:2107.06955 [Cs], July 14, 2021 [Online].</p><p>    Available: http://arxiv.org/abs/2107.06955.</p><p></p><p>[Guo et al., 2022] Yu Guo, Zhengyi Ma, Jiaxin Mao, Hongjin Qian,</p><p>    Xinyu Zhang, Hao Jiang, Zhao Cao, and Zhicheng Dou. 2022.</p><p>    Webformer: Pre-training with Web Pages for Information Retrieval.</p><p>    In Proceedings of the 45th International ACM SIGIR Conference on</p><p>    Research and Development in Information Retrieval (SIGIR '22).</p><p>    Association for Computing Machinery, New York, NY, USA, 1502-1512.</p><p>    https://doi.org/10.1145/3477495.3532086</p><p></p><p></p></body></html>